Title: Path planning for indoor Mobile robot based on deep learning
Abstract
This paper aims to give an optimal path planning of a mobile robot in a known indoor environment. An algorithm based on deep learning, ray tracing algorithm, waiting rule, and Rapidly-exploring Random Tree is proposed to solve the problem of obstacle avoidance and path planning. Firstly, GoogLeNet is used to classify obstacles. Here, it helps to distinguish between static and dynamic obstacles. Secondly, for the static obstacle avoidance, the ray tracing algorithm is proposed to avoid the obstacles which are identified by GoogLeNet. And for the dynamic obstacle avoidance, this paper proposed a waiting rule for dynamic obstacle avoidance. Thirdly, the RRT method plans a path from the start point to the goal point. The novelty of this paper is that the type of obstacles is distinguished by deep learning, and the two different kinds of obstacles used ray tracing algorithm and waiting rule to avoid obstacles collision, respectively. In experimental results, rapidly-exploring random tree is compared with genetic algorithm, and particle swarm optimization method in static environment, and it is compared with artificial potential field approach in dynamic environment. Experiments are carried out in the two-dimensional environment and successfully applied to the path planning of mobile robots in multi-obstacles environment, and they prove the feasibility of the algorithm.

1. Introduction
A lot of research is going on around the globe to find a suitable intelligent control to be used for navigation of a mobile robot without human interaction. Mobile robots can perform tasks in places where humans are unreachable or dangerously unknown, and have been successfully used in many fields. Path planning is one of the most important key technologies in the field of mobile robot research [1].
Path planning means that a robot automatically searches a collision free and optimal path from the initial point to the target point in the environment. At the same time, optimization is reflected in distance, time and energy consumption, and the most commonly used criterion is the optimization of distance. The path planning environment is either static or dynamic. In static environment, the whole solution must be found before starting execution. However, for the dynamic or partially observable environments replanning are required frequently and more planning update time is needed.
The path planning problem of mobile robot can be described as following. Given a start point, a goal point, and a set of obstacles distributed in a workspace, and find a safe and efficient path for the mobile robot. Thus the robot can go from the start point to the goal point without colliding with any obstacles along the path, the obstacles include static obstacles and dynamic obstacles. Finally, the mobile robot can complete the task of navigation and obstacle avoidance.
In the past few decades, many researchers have invented a lot of traditional methods for path planning, such as ant colony algorithm [2], Artificial Potential Field (APF) method [3], genetic algorithm (GA) [4], particle swarm optimization (PSO) [5], and so on. The genetic algorithm is a global optimization algorithm, which does not fall into the fast falling trap of the local optimal solution, but the speed of genetic algorithm is slow and it has a dependency on the selection of the initial population. The ant colony algorithm has a large amount of computation, a slow convergence rate, and is easy to fall into local optimum. Particle swarm optimization has a fairly fast approximation speed, which can effectively optimize the parameters of the system. However, it is prone to premature convergence and poor local optimization.
There are always some weaknesses in the path planning of robots only by traditional methods. Therefore, in order to solve these problems, some scholars present several ways to promote the strengths and avoid the weaknesses of the traditional methods. Especially, a lot of research work has been done on neural network-based robot path planning at home and abroad, and many neural network models for path planning have been proposed. Neural network algorithm refers to the human brain’s image thinking, and it is a nonlinear fitting process, which can process and store information in parallel [6]. Chen [7] combines neural networks and genetic algorithms for mobile robot path planning, which is difficult to guarantee real-time requirements. In a different sight of view, a collision-free path based on two neural networks: principal component analysis (PCA) and a multilayer perceptron (MLP) were constructed in [8]. Furthermore, a neural network algorithm that enables robot to move safely in an unknown environment with static or dynamic obstacles was designed in [9]. The relevant parameters of the model have a great influence on the performance of the planner, and the setting of these parameters has no effective method, and can only be tested repeatedly. Since the goal of mobile robot is to move towards an unstructured, unknown natural environment, robot must have good environmental adaptability and autonomy, and be able to react in real time in a dynamic environment.
A real-time collision-free path planning solution for 3D navigation in complex dynamic environments is proposed by Sanchez-lopez [10], and high-level geometric primitives are employed to compactly represent the environment. In [11], seven performance indicators for evaluating the results of path planning are defined.
The drawback of neural network is that as the number of neural network layers deepens, there are three major problems: non-convex optimization problems, gradient disappearance problems, and over-fitting problems. Deep learning adopts a similar hierarchical structure of neural networks, and contains multiple hidden layers. The features do not need to be manually selected, but are selected by the network itself.
Deep learning network structure is the best simulation of the human brain cortex. Convolutional neural network (CNN) is a kind of feed forward neural network with convolutional computation and deep structure. It is one of the representative algorithms of deep learning [12]. CNN has been successfully used in image classification [13]. The focus of this paper is on applying deep neural network in robot path planning.
The Rapidly-exploring Random Tree (RRT) is a randomized algorithm to explore large state space in a relatively short time, and it can be developed for robot motion planning in high dimensional configuration spaces.
The difficulty of robot path planning lies in how to intelligently plan the path planning process and complete obstacle avoidance in this process. The method of this paper identifies the types of obstacles on the basis of deep learning, and then uses the idea of ray tracing to avoid obstacles. Finally, the path planning is completed by RRT method to automate the whole process.
The contributions of this paper are 1) the obstacle identification is realized by deep learning, and the obstacles in room is divided into two kinds (static obstacle and dynamic obstacle); 2) based on the obstacle identification step, the avoidance of static obstacle is using the thought of ray tracing, and the avoidance of dynamic obstacle is using the proposed waiting rule; 3) after obstacles avoidance, the path planning of mobile robot is using the RRT algorithm. In a word, deep learning tools help robot to classify indoor obstacles, ray tracing and waiting rules to avoid static obstacles and dynamic obstacles, respectively. The first two steps make the accuracy of the final path planning algorithm improved. In the experimental results, RRT is compared with GA and PSO method in static environment, and it is compared with APF in dynamic environment.
The remainder of this paper is organized as follows. Section 2 provides a model description of this paper. The parts of obstacles are described in Section 3, including obstacle identification and obstacle avoidance. Section 4 describes the algorithm for path planning in this paper. Simulation results and discussions will be included in Section 5, and this section contains the experiments in static environment and dynamic environment. Section 6 will summarize our conclusions and gives the directions for our future research in this area.
Title: A Deep Learning Trained by Genetic Algorithm to Improve the Efficiency of Path Planning for Data Collection With Multi-UAV
Abstract:
To collect data of distributed sensors located at different areas in challenging scenarios through artificial way is obviously inefficient, due to the numerous labor and time. Unmanned Aerial Vehicle (UAV) emerges as a promising solution, which enables multi-UAV collect data automatically with the preassigned path. However, without a well-planned path, the required number and consumed energy of UAVs will increase dramatically. Thus, minimizing the required number and optimizing the path of UAVs, referred as multi-UAV path planning, are essential to achieve the efficient data collection. Therefore, some heuristic algorithms such as Genetic Algorithm (GA) and Ant Colony Algorithm (ACA) which works well for multi-UAV path planning have been proposed. Nevertheless, in challenging scenarios with high requirement for timeliness, the performance of convergence speed of above algorithms is imperfect, which will lead to an inefficient optimization process and delay the data collection. Deep learning (DL), once trained by enough datasets, has high solving speed without worries about convergence problems. Thus, in this paper, we propose an algorithm called Deep Learning Trained by Genetic Algorithm (DL-GA), which combines the advantages of DL and GA. GA will collect states and paths from various scenarios and then use them to train the deep neural network so that while facing the familiar scenarios, it can rapidly give the optimized path, which can satisfy high timeliness requirements. Numerous experiments demonstrate that the solving speed of DL-GA is much faster than GA almost without loss of optimization capacity and even can outperform GA under some specific conditions.

Introduction
A. Background and Motivation
In urban scenarios, it is very convenient to collect data from sensor nodes because of the dense deployment of communication base stations, cable networks and so on. Data can be sent from sensor nodes to the base stations and finally transmitted to the data center in IoT application scenarios. However, in some challenging scenarios without the infrastructure networks like desert and ocean [26], collecting data with unmanned aerial vehicle (UAV) [1]–[3] is obvious the choice of solution. In such data collection scenarios, the process of collecting data from a sensor node is shown in FIGURE 1 (a). UAV will approach to the sensor node and then it will hover over them and use near-field communication protocols such as RFID and Bluetooth to connect with them to collect data. Finally, UAV will leave sensor node. (As the bandwidth and data size vary according to the actual situation of the application system, the data transmission time and energy consumption during the data collection are relatively complex. In addition, in this paper, we focus on improving efficiency of path planning for multi-UAV. So, we do not consider the time and energy consumption in the process of hovering to collect data and the physical communication channel is ignored in this paper.)
In the aforementioned scenarios, there are many sensor nodes deployed at different positions. When UAVs collect data, we may just need the data from a few certain sensor nodes according to our requirements. Therefore, we will determine which the sensor nodes need to be collected by UAVs. In other words, just a part of sensor nodes needs to be collected by UAVs, which means they have demand to return data (These sensor nodes are called data nodes.). And those sensor nodes that are not selected by us will not be taken into consideration during the data collection, which means they have no demand to return data. The destination of UAVs is to collaborate to cover all the data nodes in the scenarios with the pre-designed path, whose process is shown as FIGURE 1 (b). (Here, due to the complex concepts of sensor nodes and date nodes, we clarify the difference of them. In FIGURE 1 (b), there are 5 sensor nodes in the scenario. However, only 4 sensor nodes (1,2,4 and 5 th sensor nodes) have the demand to return data, which are called data nodes and will be selected to be collected by UAVs (1,2,4 and 5 th sensor nodes are data nodes). And 3 th sensor node has no demand to return data. Therefore, it is a sensor node but not a data node. All the sensor nodes that have demand to return data will be called data nodes in the rest of this paper.)
However, without a reasonable path to collect data, required resource such as number and energy of UAVs will be increased, causing the inefficient data collection. So, an efficient optimization algorithm is necessary to decide the proper path to collect data. Since not each sensor node has the demand to return data, the algorithm needs to be executed to plan the path every time UAVs collect data. Based on the aforementioned analysis, we know that this problem has high requirement for timeliness. To conclude, we know that there are two key requirements for our algorithm, strong optimization capacity and fast convergence speed (or short solving time).
B. Limitation of Related Work
Multi-UAV path planning is one of the key challenges in the process of data collection. And there are several methods for multi-UAV path planning. Song et al. [4] proposed a cellular automata to generate coverage trajectory of UAV with given width of footprint on the ground. But this research focused on designing a path to cover an area, not points. Furthermore, it did not take into account the limitation of energy of UAVs. Sun et al. [26] proposed an optimal 3D-trajectory design and resource allocation for solar-powered UAV communication systems, which aimed at design a 3D-trajectory and allocate resource for UAV to provide the communication services in the challenging scenarios. Cai et al. [27] also proposed a trajectory and resource allocation design for energy-efficient secure UAV communication systems to achieve the maximum system energy efficiency. Although they did not ignore the energy consumption of UAV and achieve great results, they were also not suitable for our problem because they both focused on designing the trajectory of UAV to achieve the optimal area coverage and resource allocation with maximum efficiency. And there are many other proposed algorithms to dynamically control UAVs [5], [6]. But these methods are not feasible for our problem.
Genetic Algorithm (GA) and Ant Colony Algorithm (ACA) are two of the classical heuristic algorithms, which are usually used to plan the path of UAVs to cover all the points in the specific scenarios. Chen et al. [7] proposed a GA to minimize total distance of UAV. And Daryanavard and Harifi [8] also proposed an ACA to solve this problem. Besides, there are many different improved GA and ACA [9], [10] to achieve better results. However, no matter GA, ACA or their improved methods, the key shortcoming is their poor performance of convergence speed, which will cause the waste of much time and make it not suitable to optimize multi-UAV’s path for data collection that has the high requirement for timeliness. Li and Wu [11] proposed a deep reinforcement learning, which considered the deep neural network as the optimizer and train it by the reward through the interaction with environment. Once deep neural network is trained, it will rapidly give the results of path planning without the convergence process during the optimization. Therefore, the solving time of optimization process of deep reinforcement learning is much less than GA and ACA, which can satisfy the requirement for timeliness. Nevertheless, deep reinforcement learning is not the supervised learning. Before deep neural network converges, it usually takes many steps to do the random explorations, which will cost much time during training.
In general, for our problem, multi-UAV path planning requires the timeliness for algorithms. Many researches do not take convergence speed into consideration. In other words, they ignore the solving time of algorithms. GA and ACA have the strong optimization capacity but their performance of convergence speed is poor, which makes them not suitable for this problem. Although deep reinforcement learning can rapidly give the multi-UAV path planning results, its training time is not acceptable.
Thus, building a model to resolve multi-UAV path planning to collect data in the challenging scenarios to achieve shortest path and shorter solving time is valuable.
C. Our Method
In the challenging scenarios, it is uncertain which sensor nodes have the demand to return data. GA and ACA have no memory mechanism. In other words, they can not learn the past path planning experiences. So, every time UAVs collect data, GA or ACA needs to be carried out with low computation efficiency. Deep neural network has the ability to learn from the past experiences. And while it is trained, no convergence process is needed during the optimization, which makes its solving time much shorter than GA and ACA. However, deep neural network needs the path planning experiences first.
So, focus on this problem, we propose a Deep Learning Trained by Genetic Algorithm (DL-GA), which considers GA as the “instructor” to provide the path planning experiences to deep neural network and guide it to learn from the experiences. We first execute GA to gain the path planning results (including states of scenario and path) and store it in an experience replay. Then, we sample from experience replay to train deep neural network. Finally, the trained deep neural network has the path planning capacity and it can give the optimized results rapidly.
D. Challenges and Contributions
There are two main challenges for the multi-UAV path planning in the challenging scenarios. First, due to the limited energy of UAV, without a proper path to collect data will not only waste time and energy but also increase the required number of UAVs to finish the data collection. Second, in the scenario, different states of all sensor nodes (their positions and which sensor nodes have demand to return data) make it have to execute the optimization algorithm to design the trajectory every time. If this algorithm converges slowly (or its solving time is long), then the timeliness requirement cannot be satisfied. It is very complicated to figure out an algorithm with short solving time and strong optimization capacity at the same time.
Based on the above challenges, we propose a DL-GA, where GA will obtain the path planning experiences and then guide deep neural network to learn from it. This algorithm makes path planning do not need any convergence process during optimization and retain the optimization ability from GA, which satisfy the timeliness requirement. Numerical simulation results in section VI will show the superiority of DL-GA.
Our proposed algorithm focuses on covering points in the scenario by multi-UAV with the optimal path. By contrast, [4], [26], [27] both pay attention to design a trajectory for UAV to achieve the optimal area coverage. Compared with [5] and [6], they aim at designing a dynamically control method for UAV. And [7]–[10] are the different heuristic algorithms to solve the UAV path planning. However, their convergence speed is not ideal in practical applications. On the contrary, our algorithm can achieve 300 to 2000 times faster than GA and the optimization capacity can outperform GA under some specific conditions, which makes our algorithm more suitable to solve the multi-UAV path planning for data collection in the challenging scenarios. And finally, [11] proposed a deep reinforcement learning, which will take many steps to train the deep neural network and waste more time compared with our algorithm.
The rest of this paper is organized as follows. Multi-UAV path planning based on GA and DL-GA will be presented respectively in section II and section III. Then, the simulation results will be discussed to prove the performance of DL-GA in section IV. And section V will conclude this paper.
Title: Application of deep reinforcement learning in mobile robot path planning
Abstract:
In order to make the robot obtain the optimal action directly from the original visual perception without any hand-crafted features and features matching, a novel end-to-end path planning method-mobile robot path planning using deep reinforcement learning is proposed. Firstly, a deep Q-network (DQN) is designed and trained to approximate the mobile robot state-action value function. Then, the Q value corresponding to each possible mobile robot action (i.e., turn left, turn right, forward) is determined by the well trained DQN, here, the input of the DQN is the original RGB image (image pixels) captured from the environment without any hand-crafted features and features matching; Finally, the current optimal mobile robot action is selected by the action selection strategy. Mobile robot reach to the goal point while avoiding obstacles ultimately. 30 times path planning experiments are conducted in the seekavoid_arena_01 environment on DeepMind Lab platform. The experimental results show that our deep reinforcement learning based robot path planning method is an effective end-to-end mobile robot path planning method.
Introduction
Path planning problem can be described as the task of navigating a mobile robot around a space in which a number of obstacles that should be avoided. The task usually is under some optimization criteria, such as least working cost, shortest walking distance, minimal walking time, etc. It is an important and challenging topic in robotics [1]. In many applications of robots, the working environments are complex and unpredictable, which require the path planning methods to show self-study, adaptation and robust abilities. To overcome the weakness of these approaches, researchers explored variety of solutions. Reinforcement learning (RL) techniques can learn appropriate actions from the environment states, whose benefits are based on the concept of online learning, and rewards or punishments from the environments. Therefore, the agent is allowed to modify its policy from the rewards or punishments it receives. Presently, reinforcement learning algorithms have been well applied in mobile robot path planning problems and achieved important achievements [2].
However, traditional RL_based path planning methods heavily rely on hand-crafted features of the task representation, which are not powerful for raw high dimensional input images, so, learning a control strategy from a raw image directly is still an important challenging for RL[3], [4]. Recent advances in deep learning have made it possible to exploit high-level feature from raw image data, leading to great breakthroughs in speech recognition, computer vision and other applications [5].
Specifically, Mnih et al. [5] from Google Deepmind team combined the convolutional neural network and Q-learning of the Traditional RL, then proposed a deep Q-network model to solve the high dimensional perception based decision problem. They utilize a deep Q-network (DQN) to evaluate the Q-function for Q-learning. Many followed papers tried to make improvements from the model of Mnih et al [5], as DQN is essentially state-of-the art and was the main catalyst for deep RL [5], [6].
Based on this, in this paper, deep RL technology is applied in mobile robot path planning and achieve the end-to-end mobile robot path planning, that is, the proposed planning method can determine the optimal action to make the mobile robot reach to the goal point while avoiding obstacles only using the original visual perception without any hand-crafted features and features matching.
The paper is organized as follows: The general framework of the proposed path planning method and the basic implement principle of the each components are introduced in section 2; In section 3, some experiment results and analysis are presented. Finally, some conclusion and improvement issues for future research are described in section 4.
Title: Tool path planning of consecutive free-form sheet metal stamping with deep learning
Abstract
Sheet metal forming technologies, such as stamping and deep drawing, have been widely used in automotive, rail and aerospace industries for lightweight metal component manufacture. It requires specially customised presses and dies, which are very costly, particularly for low volume production of extra-large engineering panel components. In this paper, a novel recursive tool path prediction framework, impregnated with a deep learning model, is developed and instantiated for the forming sequence planning of a consecutive rubber-tool forming process. The deep learning model recursively predicts the forming parameters, namely punch location and punch stroke, for each deformation step, which yields the optimal tool path. Three series of deep learning models, namely single feature extractor, cascaded networks (including state-of-the-art deep networks) and long short-term memory (LSTM) models are implemented and trained with two datasets with different amounts of data but the same data diversity. The learning results show that the single LSTM model trained with the larger dataset has the most superior learning capability and generalisation among all models investigated. The promising results from the LSTM indicate the potential of extending the proposed recursive tool path prediction framework to the tool path planning of more complex sheet metal components. The analysis on different deep networks provides instructive references for model selection and model architecture design for sheet metal forming problems involving tool path design.
1. Introduction
Sheet metal forming technology has been developed for centuries for manufacture of lightweight metal components, which has been vastly employed in the automotive and aerospace industries as of today. Among all the sheet metal forming techniques, cold stamping is believed to be the most commonly used one (Zheng et al., 2018). Owing to its short forming cycle, stamping is widely used for mass production. However, subject to its one-step forming pattern, the forming flexibility of this technique is limited. In addition, high capital cost on customised punch and dies can shrink the profit margins of products, especially for low volume production. As a consequence, these constraints necessitate the development of consecutive free-form stamping processes which could have the same forming results as one-step stamping, and the optimisation of the tool path plays a dominant role in the development.
Over the last two decades, machine learning technology, especially deep learning, has seen its resurgence and burgeoning development in various areas thanks to the revolutionarily increased computation power of computer processors. The areas receiving the most benefits from machine learning include Computer Vision, National Language Processing (NLP) and Medical Imaging. For example, Zhao et al. (2019) introduced the powerful capability of deep neural networks in learning semantic, high-level and deeper features from images by reviewing different network architectures for object detection. Cambria and White (2014) reported that the speed of NLP analysis of a sentence had been boosted from 7 min for a sentence to less than a second, with the rapid development of deep learning. A quick advancement of Medical Image Analysis (MIA), especially in identifying, classifying and measuring patterns in medical images, was also brought by the recent improvements of deep learning, as suggested by Buettner et al. (2020). Machine learning algorithms are commonly classified into three taxonomies: supervised learning, unsupervised learning and reinforcement learning (Monostori et al., 1996). In sheet metal forming industry, supervised learning dominates the machine learning related applications.
As of today, the most prevailing machine learning applications to sheet metal stamping include process monitoring, fault diagnosis and surrogate assisted optimisation. In terms of process monitoring and fault diagnosis, machine learning models are developed to predict the process condition or manufacturing defects based on process-relevant parameters. For example, García (2005) used two separate multilayer perceptrons (MLPs) for wrinkles and cracks detection in a sheet metal stamping process. The MLPs were embedded in a coupled sensors-based monitoring system which proved to be useful for defects detection of deep drawing parts. Chen et al. (2019) analysed the punch sounds from stamping process with a 5-layer neural network to estimate the life period of a stamping press. They constructed a prototype which had been proved to realise the real-time estimation. Similarly, Huang and Dzulfikri (2021) developed a one-dimensional convolutional neural network (CNN) to monitor the tool health during the real-time stamping process. In their work, the vibration signal during the stamping process was measured for training the CNN to predict the class of the tool wear condition. For surrogate assisted optimisation, an end-to-end prediction is realised by the machine learning model for fast forming results estimation, with which shorter optimisation cycle can be achieved. Hart-Rawung et al. (2020) exploited a deep neural network (DNN) with 50 hidden layers as the surrogate model to replace the traditional austenite decomposition model for faster hot stamping simulations. The results showed that the surrogate model managed to predict the final phase fraction within the desired accuracy. Zhou et al. (2022) compared the performance of a U-Net based surrogate model and an MLP based surrogate model in predicting the plastic strain fields of stamping simulations. Instead of using traditional hand-crafted features to describe the workpiece geometry, they used image-based inputs to describe the design parameters and found that the image-based method is advantageous over the traditional method in accuracy, generalisability, robustness and informativeness. For more state-of-the-art surrogate assisted methods, Wang et al. (2017b) surveyed on a series of models and discussed the performance and potential development of these methods in sheet metal forming design.
Although many studies have been reported on machine learning applications to metal forming, few has focused on tool path prediction of a consecutive sheet metal stamping. Most of the path generation optimisations with machine learning are applied to incremental sheet metal forming (ISF) technique because of its continuous deformation process. Hartmann et al. (2016) developed a neural network, which is composed of four neural layers, to learn the desired workpiece shape and predict the optimal tool movement path in an incremental sheet metal free-forming process. The input to the network is a processed form of target workpiece geometry, they claimed that the forms of inputs and outputs have to be carefully designed to achieve good learning efficiency. Störkle et al. (2016) exploited a reinforcement learning algorithm to adjust the tool path in an ISF process and managed to increase the geometric accuracy of the products. Opritescu and Volk (2015) established a shallow neural network to predict the tool path strategy for a driving process, which deformed an L-shaped sheet metal by local material stretching and shrinking. The sheet metal shape deformed by the strategy from neural network was found to be highly accurate only for short profile lengths. Liu et al. (2020) developed an overall learning algorithm with the incorporation of a reinforcement learning algorithm for the prediction of optimal tool path of a free-form sheet metal stamping process. However, a shared issue hindering applications of reinforcement learning in engineering problems is that the computation for finite element analysis (FEA) of each forming operation can be prohibitively expensive. A sophisticated fast FE simulation approach, such as Knowledge-Based Cloud FE simulation method developed by Wang et al. (2017a), is to be developed for reducing computation requirements. As a consequence, due to the huge path search space for complex component deformation, the slow learning process of machine learning models may not guarantee the discovery of an optimal tool path.
The aim of this research is to fill in the gap of intelligent path planning for consecutive free-form stamping. In this paper, a recursive tool path prediction framework was proposed, with a deep learning model as the core component. This method was instantiated on a 2-dimensional (2-D) consecutive rubber-tool forming process, which is a forming technique designed for producing sheet metals with Class A surface condition. Three different series of deep learning models, namely single feature extractor, cascaded network and long short-term memory (LSTM) models, were developed compared in predicting the optimal tool path and the corresponding forming parameters at each forming step, based on their topology learning from the shape discrepancy between the input workpieces. Two forming parameters, punch location and punch stroke, were used to quantitatively describe the forming operation. For more comprehensive comparison, two datasets with maximum deformation times of two and three were used for training the deep learning models.
The main contributions of this paper are as following: 1) filling the gap of tool path planning for free-form sheet metal stamping process with deep learning technologies in metal forming industry; 2) proposing a novel heuristics-respected recursive tool path prediction framework, with which the performances of several adapted state-of-the-art deep learning networks in tool path prediction for a free-form sheet metal stamping process were compared; 3) providing instructive references for model selection and model architecture design for sheet metal forming problems involving tool path design.
Title: Performance Improvement of Path Planning algorithms with Deep Learning Encoder Model
Abstract:
Currently, path planning algorithms are used in many daily tasks. They are relevant to find the best route in traffic and make autonomous robots able to navigate. The use of path planning presents some issues in large and dynamic environments. Large environments make these algorithms spend much time finding the shortest path. On the other hand, dynamic environments request a new execution of the algorithm each time a change occurs in the environment, and it increases the execution time. The dimensionality reduction appears as a solution to this problem, which in this context means removing useless paths present in those environments. Most of the algorithms that reduce dimensionality are limited to the linear correlation of the input data. Recently, a Convolutional Neural Network (CNN) Encoder was used to overcome this situation since it can use both linear and non-linear information to reduce data. This paper analyzes in-depth the performance to eliminate the useless paths using this CNN Encoder model. To measure the mentioned model efficiency, we combined it with different path planning algorithms. Next, the final algorithms (combined and not combined) are checked in a database composed of five scenarios. Each scenario contains fixed and dynamic obstacles. Their proposed model, the CNN Encoder, associated with other existent path planning algorithms in the literature, was able to obtain a time decrease to find the shortest path compared to all path planning algorithms analyzed. the average decreased time was 54.43%.
Introduction
Path planning algorithms are essential for the accomplishment of many activities in different areas, for example, robot navigation [1], path apps for locomotion in cities (for pedestrian and driver) [2], autonomous-driver cars [3]. These algorithms have different approaches to treat spatial information, the most used in the literature are, Grid-based search (which transforms the environment in a grid-mesh) [4], Interval-based search (similar to grid-based search it but uses space data instead of a grid) [5] and Reward-based (similar to a reinforcement learning in deep learning) [6].
Based on Grid-based search, the first path planning algorithm was proposed by Dijkstra in 1956. Although this solution is always able to find the shortest path between two points, Dijkstra's algorithm has become obsolete because it has very high computational complexity. Considering the Dijkstra algorithm's response time, it would be infeasible to be applied in many scenarios.
Given this problem, new algorithms with different approaches were created to improve performance in finding the shortest path between two points.
Since Dijkstra's proposal, many algorithms have been created being able to find the shortest path with the lowest computational cost. A* [4], Bi A* [7], Breadth-first [8], Best-First [9] are a few of the search algorithms that exist for path planning. They have peculiarities that tackle different problems, and, therefore, are useful and important in many areas. However, these algorithms become extremely costly when applied to large environments or environments with dynamic objects [10]. All these algorithms are detailed in the theoretical foundation section II-A.
The problem of the increased computational cost when increasing the amount of information is a problem that affects several fields of research, for example, pattern recognition [11], computer vision [12], text mining [13]. A very well-known approach to avoid this problem is to reduce dimensionality by discarding irrelevant information to the task.
Principal component analysis (PC A) is a mathematical procedure based on orthogonal transformation to convert data into a set of values of linearly unrelated variables called principal components. The number of principal components is always less than or equal to the number of original variables [14].
Truncated Singular value decomposition (TSVD) This algorithm use means of TSVD to performs linear dimensionality reduction. Differently of PCA, this solution does not center the data before computing the singular value decomposition [15].
Non-negative matrix factorization (NMF) Creates two non- negative matrices (W, H). The product of these matrices is an approximation of the non-negative input data. This method is used for dimensionality reduction [16].
These solutions significantly reduce dimensionality, keeping enough information to accomplish some tasks. However, a limitation of these approaches is since they reduce dimensionality using only linear correlation [17]. Therefore, in [18], the authors built a deep learning model able to reduce dimensionality using non-linearity correlation, named Convolutional Neuronal Network (CNN) Encoder. Their method removes mostly the useless information of the input data, including in dynamic environments. Eliminating useless information for the path planning problem is not just about removing paths that do not connect the starting point and the objective point, but also not considering the paths that make the course longer.
Given the good results presented in our previous work, we performed an in-depth analysis of the integration between CNN Encoder and path planning algorithms already disseminated in the literature. Then, a statistical analysis was performed to all the algorithms explored in this work, in which the focus will be the execution time of the task in different scenarios. Thus, it becomes possible to prove the efficiency of the solution and validate its use in parallel to the different solutions applied, thus constituting a robust framework to solve problems related to path planning.
Title: Robot path planning based on deep reinforcement learning
Abstract:
Q-learning algorithm based on Markov decision process as a reinforcement learning algorithm can achieve better path planning effect for mobile robot in continuous trial and error. However, Q-learning needs a huge Q-value table, which is easy to cause dimension disaster in decision-making, and it is difficult to get a good path in complex situations. By combining deep learning with reinforcement learning and using the perceptual advantages of deep learning to solve the decision-making problem of reinforcement learning, the deficiency of Q-learning algorithm can be improved. At the same time, the path planning of deep reinforcement learning is simulated by MATLAB, the simulation results show that the deep reinforcement learning can effectively realize the obstacle avoidance of the robot and plan a collision free optimal path for the robot from the starting point to the end point.
Introduction
Mobile robot is a kind of mechanical device which can perform work automatically, it can replace human beings to complete many monotonous and repetitive tasks. With the continuous development of automation technology, mobile robot has been widely used in agriculture, military, medical and other fields with its good applicability. The path planning of mobile robot is always the key to complete the task, it means that the robot can find a collision free optimal path from the starting point to the end point in the complex environment. Good path planning can improve the working efficiency of the robot and reduce unnecessary excess.
Common path planning algorithms include artificial potential field[1], A-star algorithm[2], Rapidly-Exploring Random Trees[3], ant colony optimization[4], Particle Swarm Optimization[5] and so on. Traditional algorithms need complete environment information, which makes it difficult for the robot to make good path planning through unsupervised learning and supervised learning in the absence of prior knowledge.
Reinforcement learning is a kind of trial and error learning, agents get reward feedback through continuous interaction with environment to find the optimal strategy or the maximum positive reward[6]. Q-learning algorithm is a typical model independent reinforcement learning algorithm, the convergence can be guaranteed without knowing the model. It is one of the most effective reinforcement learning algorithms applied to robot path planning, which can get good path planning in the case of small state space [7]. But Q-learning algorithm needs to build a huge Q-value table in complex environment, this method is not only time-consuming but also slow in convergence, which is easy to have dimension disaster[8]. Therefore, Q-learning is difficult to apply to the path planning of mobile robot in large state space. By combining convolution neural network with reinforcement learning and using the output of neural network instead of Q-value table, the problem of dimension disaster and convergence speed can be effectively solved.
Title: Deep Learning Enabled Fine-Grained Path Planning for Connected Vehicular Networks
Abstract:
In this paper, to alleviate the ever-increasing traffic congestion in urban areas by accommodating higher road traffic, we develop traffic prediction framework together with path planning method for connected vehicular networks. First, through the employment of convolutional neural network (CNN) and residual unit (RN), deep learning (DL) based fine-grained traffic prediction algorithm is designed to obtain the spatial-temporal characteristics of vehicular traffic. The regionally fine-grained traffic prediction framework can realize real-time traffic prediction of future changing trends at each road with a high accuracy and reliability. Second, we propose a gridded path planning method by making use of the traffic prediction information. The accuracy of selected path, complexity of path calculation, and adaptive path adjustment are jointly taken into consideration by achieving the refined traffic regulation in different gridded section. Finally, we utilize the actual vehicle data from the city of Beijing and digital map on OpenStreetMap to validate the effectiveness and reliability of the proposed traffic prediction framework and path planning method. Simulation results demonstrate that the proposed approach is capable of relieving urban traffic congestion based on the existing roadway systems, which can provide methodological guidance for data-intensive traffic management.
Introduction
With the continuously increasing amount of private vehicles in urban areas, alleviating traffic congestion has become one of the most challenging issues for providing efficient traffic management and convenient travel experience [1], [2]. According to the statement of Urban Mobility Scorecard, traffic congestion in the United States causes people to spent nearly 6.8 billion hours more on travelling each year, aligned with a total economic cost estimated at 160 billion [3], [4]. Traditional attempt for relieving congestion situation is to expand traffic capacity by primarily broadening road space or building more transport infrastructures. However, due to the limited space in metropolitan city and the surging traffic that can easily saturate the road capacity, long-standing congestion continues to be the bane of many urban areas for decades [5]. As wireless communication significantly develops in recent years, cooperative traffic management solution via vehicle-to-everything (V2X) technologies open up a new window of opportunity to tame congestion based on the current layout of transportation system [6]–[8]. Besides, the advent of connected vehicles (CVs) equipped with intelligent cameras and sensors further enriches the way to obtain real-time traffic data, which makes the information intensive traffic regulation high on the list of possibilities [9]–[11].
Although the advancement of wireless communication and vehicular technologies is capable of collecting and transmitting sufficient traffic information, how to deal with sophisticated data processing problem under individualized scheduling task is still a matter of great difficulty [12]–[14]. With recent artificial intelligent (AI) technologies applied into the filed of intelligent transportation system (ITS), deep learning (DL) based traffic prediction has attracted widespread attention from both academia and industry [15]. On the one hand, urban traffic possesses salient characteristics such as local coherence and flow periodicity, which means that the urban traffic can be predicted beforehand by learning the spatio-temporal correlations from historical traffic observation [16]. On the other hand, classical artificial neural network in DL, like convolution neural network (CNN), can automatically and hierarchically capture the structural information of traffic flow through convolution operations [17]. Therefore, when it comes to the multi-dimensional data processing and computing for traffic prediction task, various neural networks can be leveraged to attain the changing trends of future traffic, with a high level of accuracy and reliability [18].
After obtaining the traffic prediction information, effective exploitation of the information can provide powerful assistance for creative data-driven scheduling strategies through taking current traffic situations and future changing trends into consideration [19]. Among all the potential solutions of relieving traffic congestion, the path planning, acting as a kind of effective method to prevent selfish driving choices which can worsen the inefficient traffic network, has gained lots of popularity for many classic algorithms such as Dijkstra algorithm [20] and A* algorithm [21]. Based on the acquired traffic information, path planning algorithm plays an a decisive role in the performance of traffic management efficiency [22]. However, most of the existing path planning methods are usually accompanied with remarkable computation complexity, especially under the city-scale path planning situation. Moreover, due to the dynamic changing environment brought by high-speed mobility of vehicles, it is difficult for them to satisfy the requirement of real-time adjustment so as to get with people’s intention to travel as well as their associated travel behavior.
With the above-mentioned considerations, we first present a DL based fine-grained traffic prediction method to obtain the future traffic information of each urban road. After that, to improve the effectiveness of path planning and ensure low computation complexity, an innovative gridded path planning algorithm based on predicted traffic information is proposed to realize the refined vehicle scheduling in different gridded area. The main contributions of this paper are summarized as follows:
We propose a fine-grained grid-level traffic prediction framework to realize the real-time traffic prediction of each road with a high level of accuracy and reliability, through which can significantly reduce the complexity of traffic prediction in large-scale road networks and provide effective guidance on vehicular scheduling.
We introduce a gridded path planning method through jointly considering the accuracy of selected path, complexity of path calculation and real-time path adjustment, which can provide general and effective guidance on realtime vehicular path planning for some suddenly emerged traffic scenarios, such as traffic emergencies, changeable weather, frequent road construction, etc.
We verify the proposed fine-grained traffic prediction framework and gridded path planning method using the actual vehicle data in practical traffic system and digital map from OpenStreetMap. Simulation results confirm that our solution can promote efficient usage of roadway systems and better deal with congested problem.
The rest of this paper is organized as follows. Section II introduces the related work of traffic prediction and path planning. The system model is discussed in Section III. Section IV presents the proposed fine-grained traffic prediction framework and gridded path planning is describled in Section V. Section VI demonstrates the simulation results. Finally, this paper is concluded in Section VII.